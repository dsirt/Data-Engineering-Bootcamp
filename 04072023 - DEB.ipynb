{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d043ea-8c32-4e77-9859-30369a3637c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |William |36636|M     |3000  |\n|Michael  |Smith     |        |40288|M     |4000  |\n|Robert   |          |Dawson  |42114|M     |4000  |\n|Maria    |          |Jones   |39192|F     |4000  |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName('ProjectFirst') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"\", \"William\", \"36636\", \"M\", 3000), (\"Michael\", \"Smith\", \"\", \"40288\", \"M\", 4000), (\"Robert\", \"\", \"Dawson\", \"42114\", \"M\", 4000), \n",
    "        (\"Maria\", \"\", \"Jones\", \"39192\", \"F\", 4000)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\\\n",
    "    StructField(\"middlename\", StringType(), True),\\\n",
    "    StructField(\"lastname\", StringType(), True),\\\n",
    "    StructField(\"id\", StringType(), True),\\\n",
    "    StructField(\"gender\", StringType(), True),\\\n",
    "    StructField(\"salary\", IntegerType(), True)\\\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394b5bc2-e1d4-4771-91d4-d8f24ee39c57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Dogu         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName('ProjectSecond') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),\\\n",
    "    (\"Michael\", \"Sales\", 4600),\\\n",
    "    (\"Robert\", \"Sales\", 4100),\\\n",
    "    (\"Maria\", \"Finance\", 3000),\\\n",
    "    (\"James\", \"Sales\", 3000),\\\n",
    "    (\"Scott\", \"Finance\", 3300),\\\n",
    "    (\"Jen\", \"Finance\", 3900),\\\n",
    "    (\"Jeff\", \"Marketing\", 3000),\\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Dogu\", \"Sales\", 4100)]\n",
    "\n",
    "column = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = column)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033c7a6b-db54-4457-8762-51fd44b788e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Michael      |Sales     |4600  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Jen          |Finance   |3900  |\n|Scott        |Finance   |3300  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|Dogu         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct Count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Michael      |Sales     |4600  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Jen          |Finance   |3900  |\n|Scott        |Finance   |3300  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|Dogu         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct Count: 8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Michael      |Sales     |4600  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Distinct\n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct Count: \" + str(distinctDF.count()))\n",
    "distinctDF.show(truncate = False)\n",
    "\n",
    "#Drop Duplicates\n",
    "df2 = df.dropDuplicates()\n",
    "print(\"Distinct Count: \" + str(df2.count()))\n",
    "df2.show(truncate = False)\n",
    "\n",
    "dropDisDF = df.dropDuplicates([\"department\", \"salary\"])\n",
    "print(\"Distinct Count: \" + str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f829d6-bfd6-44da-b5a9-b29bafe573d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |William |36636|M     |3000  |\n|Michael  |Smith     |        |40288|M     |4000  |\n|Robert   |          |Dawson  |42114|M     |4000  |\n|Maria    |          |Jones   |39192|F     |4000  |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName('ProjectFirst') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"\", \"William\", \"36636\", \"M\", 3000), (\"Michael\", \"Smith\", \"\", \"40288\", \"M\", 4000), (\"Robert\", \"\", \"Dawson\", \"42114\", \"M\", 4000), \n",
    "        (\"Maria\", \"\", \"Jones\", \"39192\", \"F\", 4000)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\\\n",
    "    StructField(\"middlename\", StringType(), True),\\\n",
    "    StructField(\"lastname\", StringType(), True),\\\n",
    "    StructField(\"id\", StringType(), True),\\\n",
    "    StructField(\"gender\", StringType(), True),\\\n",
    "    StructField(\"salary\", IntegerType(), True)\\\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22f9b94-b38d-4f4f-9020-4d4284479b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  firstname middlename lastname     id gender  salary\n0     James             William  36636      M    3000\n1   Michael      Smith           40288      M    4000\n2    Robert              Dawson  42114      M    4000\n3     Maria               Jones  39192      F    4000\n"
     ]
    }
   ],
   "source": [
    "PandasDF = df.toPandas()\n",
    "print(PandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca514ad4-b095-4345-b589-31a0d3eda120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-------+\n|Product|Amount|Country|\n+-------+------+-------+\n|Banana |1000  |USA    |\n|Carrots|1500  |USA    |\n|Beans  |1600  |USA    |\n|Orange |2000  |USA    |\n|Orange |2000  |USA    |\n|Banana |4000  |China  |\n|Carrots|1200  |China  |\n|Beans  |1500  |China  |\n|Orange |4000  |China  |\n|Banana |2000  |Canada |\n|Carrots|2000  |Canada |\n|Beans  |2000  |Mexico |\n+-------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data = [(\"Banana\", 1000, \"USA\"), (\"Carrots\", 1500, \"USA\"), (\"Beans\", 1600, \"USA\"),\\\n",
    "    (\"Orange\", 2000, \"USA\"), (\"Orange\", 2000, \"USA\"), (\"Banana\", 4000, \"China\"),\\\n",
    "    (\"Carrots\", 1200, \"China\"), (\"Beans\", 1500, \"China\"), (\"Orange\", 4000, \"China\"),\\\n",
    "    (\"Banana\", 2000, \"Canada\"), (\"Carrots\", 2000, \"Canada\"), (\"Beans\", 2000, \"Mexico\")\\\n",
    "    ]\n",
    "\n",
    "columns = ['Product', 'Amount', 'Country']\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4afd5bd-c8ca-4af8-bb36-55aac49cd161",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Product: string (nullable = true)\n |-- Canada: long (nullable = true)\n |-- China: long (nullable = true)\n |-- Mexico: long (nullable = true)\n |-- USA: long (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |4000 |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c264134-47b5-4a93-ab93-370312c9d58e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n|   TV|Radio|Newspaper|Sales|\n+-----+-----+---------+-----+\n|230.1| 37.8|     69.2| 22.1|\n| 44.5| 39.3|     45.1| 10.4|\n| 17.2| 45.9|     69.3|  9.3|\n|151.5| 41.3|     58.5| 18.5|\n|180.8| 10.8|     58.4| 12.9|\n+-----+-----+---------+-----+\nonly showing top 5 rows\n\nroot\n |-- TV: double (nullable = true)\n |-- Radio: double (nullable = true)\n |-- Newspaper: double (nullable = true)\n |-- Sales: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName('ProjectThird') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format('delta') \\\n",
    "    .options(header = 'True', inferschema = 'True')\\\n",
    "    .load(\"/user/hive/warehouse/advertising\", header = True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ba210d-4f66-44d6-a595-547c62e58bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n|col1|col2|col3| col4|\n+----+----+----+-----+\n|   1|   2|   3|a b c|\n|   4|   5|   6|d e f|\n|   7|   8|   9|g h i|\n+----+----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#RDD creation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName('ProjectRDDCreation') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sparkContext.parallelize([(1,2,3, 'a b c'), (4,5,6, 'd e f'), (7,8,9, 'g h i')]).toDF(['col1', 'col2', 'col3', 'col4'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fea6c861-1141-46bb-8284-f26f9b0aff59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformations & Actions"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04072023 - DEB",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
